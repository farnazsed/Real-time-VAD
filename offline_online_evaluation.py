# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mBqIc7l6qeNFks7lymmGXbg-v0iyQOYQ
"""

# Install required packages
!pip install pyannote.audio torchaudio matplotlib tqdm
!apt-get -qq install libportaudio2

# Authenticate with Hugging Face
from huggingface_hub import notebook_login
notebook_login()

# Now import everything else
import torch
import numpy as np
from pyannote.audio import Pipeline, Audio, Model
from pyannote.database import registry, FileFinder
from pyannote.core import Segment, Annotation, notebook, SlidingWindow, SlidingWindowFeature
from pyannote.metrics.diarization import DiarizationErrorRate
from pyannote.metrics.detection import DetectionErrorRate
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc
import time
from tqdm import tqdm
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

# Clone AMI dataset setup
!git clone https://github.com/pyannote/AMI-diarization-setup.git

# Set up the database
registry.load_database("AMI-diarization-setup/pyannote/database.yml")
preprocessors = {"audio": FileFinder()}
ami = registry.get_protocol('AMI.SpeakerDiarization.mini', preprocessors=preprocessors)



class VADEvaluator:
    def __init__(self):
        self.sample_rate = 16000
        self.chunk_size = 512
        self.vad_window = 2048
        self.hangover_time = 0.2

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Initialize pipeline with authentication
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/voice-activity-detection",
            use_auth_token=True
        ).to(self.device)

        self.pipeline.instantiate({
            'onset': 0.7, 'offset': 0.5,
            'min_duration_on': 0.1,
            'min_duration_off': 0.2
        })

        self.audio = Audio(sample_rate=self.sample_rate, mono=True)
        self.der_metric = DiarizationErrorRate()
        self.detection_metric = DetectionErrorRate()
        self.processing_times = []
        self.frame_counts = []

    def evaluate_offline(self, test_file):
        """Evaluate VAD performance on a complete audio file"""
        # Load audio and ensure proper shape (channel, time)
        waveform, _ = self.audio(test_file)
        waveform = waveform.unsqueeze(0) if len(waveform.shape) == 1 else waveform  # Ensure (channel, time)
        reference = test_file['annotation']

        # Process entire file at once
        start_time = time.time()
        vad_result = self.pipeline({
            "waveform": waveform.to(self.device),
            "sample_rate": self.sample_rate
        })
        processing_time = time.time() - start_time

        # Calculate metrics
        der = self.der_metric(reference, vad_result, uem=test_file['annotated'])
        detection_rate = self.detection_metric(reference, vad_result)

        return {
            'der': der,
            'detection_rate': detection_rate,
            'processing_time': processing_time,
            'audio_duration': waveform.shape[1]/self.sample_rate,
            'rtf': processing_time / (waveform.shape[1]/self.sample_rate),
            'waveform': waveform,
            'vad_result': vad_result,
            'reference': reference
        }

    def evaluate_online(self, test_file):
        """Simulate online processing by feeding audio in chunks"""
        waveform, _ = self.audio(test_file)
        waveform = waveform.unsqueeze(0) if len(waveform.shape) == 1 else waveform  # Ensure (channel, time)
        reference = test_file['annotation']

        # Reset timing stats
        self.processing_times = []
        self.frame_counts = []

        # Simulate online processing
        start_time = time.time()
        vad_result, frame_scores = self._process_online(waveform)
        processing_time = time.time() - start_time

        # Calculate metrics
        der = self.der_metric(reference, vad_result, uem=test_file['annotated'])
        detection_rate = self.detection_metric(reference, vad_result)

        return {
            'der': der,
            'detection_rate': detection_rate,
            'processing_time': processing_time,
            'audio_duration': waveform.shape[1]/self.sample_rate,
            'rtf': processing_time / (waveform.shape[1]/self.sample_rate),
            'avg_frame_time': np.mean(self.processing_times),
            'waveform': waveform,
            'vad_result': vad_result,
            'reference': reference
        }

    def _process_online(self, waveform):
        results = []
        frame_scores = []
        frame_times = []

        num_channels, num_samples = waveform.shape
        num_chunks = num_samples // self.chunk_size

        last_speech_end = None
        hangover_end = None

        for i in range(num_chunks):
            current_time = i * self.chunk_size / self.sample_rate
            start = i * self.chunk_size
            end = start + self.vad_window
            chunk = waveform[:, start:end]

            if chunk.shape[1] < self.vad_window:
                chunk = torch.nn.functional.pad(chunk, (0, self.vad_window - chunk.shape[1]))

            with torch.no_grad():
                vad_out = self.pipeline({
                    "waveform": chunk.to(self.device),
                    "sample_rate": self.sample_rate
                })

            speech_segments = list(vad_out.get_timeline().support())

            # Frame score: 1.0 if speech, 0.0 if silence
            frame_score = 1.0 if speech_segments else 0.0
            frame_time = current_time + (self.chunk_size / self.sample_rate) / 2
            frame_scores.append([frame_score])
            frame_times.append(frame_time)

            if speech_segments:
                for segment in speech_segments:
                    adjusted_start = segment.start + current_time
                    adjusted_end = segment.end + current_time
                    results.append((adjusted_start, adjusted_end, "speech"))

                last_speech_end = current_time + (self.vad_window / self.sample_rate)
                hangover_end = last_speech_end + self.hangover_time

            elif last_speech_end and current_time < hangover_end:
                if results:
                    last_segment = results[-1]
                    extended_end = current_time + (self.chunk_size / self.sample_rate)
                    results[-1] = (last_segment[0], extended_end, "speech")

        annotation = Annotation()
        for start, end, label in results:
            annotation[Segment(start, end)] = label

        resolution = SlidingWindow(
            start=frame_times[0],
            duration=self.chunk_size / self.sample_rate,
            step=self.chunk_size / self.sample_rate
        )
        frame_level_result = SlidingWindowFeature(np.array(frame_scores), resolution)

        return annotation, frame_level_result



    def plot_vad_comparison(self, test_file, results):
        """Plot VAD results vs ground truth and frame-level prediction"""

        # Get ground truth and VAD results
        ground_truth = test_file["annotation"]
        vad_annotation = results['vad_result']  # already an Annotation
        waveform = results['waveform']
        self.calculate_detection_metrics(
            ground_truth.get_timeline().support(),
            vad_annotation.get_timeline().support(),
            results['audio_duration']
        )

        # Create figure with 3 subplots
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 10), sharex=True)

        # Plot waveform
        waveform_time = np.linspace(0, len(waveform[0]) / self.sample_rate, len(waveform[0]))
        ax1.plot(waveform_time, waveform[0].cpu().numpy(), 'b-', alpha=0.7, linewidth=0.5)
        ax1.set_ylabel('Amplitude')
        ax1.set_title('Audio Waveform')
        ax1.grid(True, alpha=0.3)

        # Plot ground truth vs VAD result (annotation level)
        from pyannote.core import notebook
        notebook.plot_annotation(ground_truth, ax=ax2, time=True, legend=False)
        notebook.plot_annotation(vad_annotation, ax=ax2, time=True, legend=False)
        ax2.set_title('Ground Truth (Blue) vs VAD Result (Red)')
        ax2.set_xlabel('Time (seconds)')
        ax2.grid(True, alpha=0.3)

        # Plot frame-level VAD prediction (from SlidingWindowFeature)
        scores = results['vad_frame_scores']
        times = [t for t, _ in scores.itertracks()]
        values = scores.data.squeeze()

        ax3.plot(times, values, label="Online Frame-Level VAD", color='purple')
        ax3.set_xlabel("Time (s)")
        ax3.set_ylabel("Speech Probability")
        ax3.set_title("Frame-wise VAD Prediction (Online Mode)")
        ax3.set_ylim(-0.1, 1.1)
        ax3.grid(True)
        ax3.legend()

        plt.tight_layout()
        plt.show()

      # Calculate precision, recall, F1


    def calculate_detection_metrics(self, ground_truth, vad_result, audio_duration):
        """Calculate precision, recall and F1-score from segments"""
        # Convert to binary arrays at 100ms resolution
        time_resolution = 0.1  # 100ms
        num_points = int(audio_duration / time_resolution)
        time_points = np.linspace(0, audio_duration, num_points)

        # Create binary arrays
        gt_binary = np.zeros(num_points)
        vad_binary = np.zeros(num_points)

        # Mark ground truth segments
        for segment in ground_truth:
            start_idx = int(segment.start / time_resolution)
            end_idx = int(segment.end / time_resolution)
            gt_binary[start_idx:end_idx] = 1

        # Mark detected segments
        for segment in vad_result:
            start_idx = int(segment.start / time_resolution)
            end_idx = int(segment.end / time_resolution)
            vad_binary[start_idx:end_idx] = 1

        # Calculate metrics
        true_positives = np.sum((gt_binary == 1) & (vad_binary == 1))
        false_positives = np.sum((gt_binary == 0) & (vad_binary == 1))
        false_negatives = np.sum((gt_binary == 1) & (vad_binary == 0))

        precision = true_positives / (true_positives + false_positives + 1e-8)
        recall = true_positives / (true_positives + false_negatives + 1e-8)
        f1_score = 2 * (precision * recall) / (precision + recall + 1e-8)

        print("\nDetection Metrics:")
        print(f"Precision: {precision:.3f}")
        print(f"Recall: {recall:.3f}")
        print(f"F1-score: {f1_score:.3f}")
        print(f"True Positives: {true_positives}")
        print(f"False Positives: {false_positives}")
        print(f"False Negatives: {false_negatives}")

def run_full_evaluation():
    evaluator = VADEvaluator()
    test_files = list(ami.test())[:3]  # Evaluate on first 3 files

    offline_results = []
    online_results = []

    print("Running offline evaluation...")
    for test_file in tqdm(test_files, desc="Processing files"):
        result = evaluator.evaluate_offline(test_file)
        offline_results.append(result)

    print("\nRunning online evaluation...")
    for test_file in tqdm(test_files, desc="Processing files"):
        result = evaluator.evaluate_online(test_file)
        online_results.append(result)

    # Print summary statistics
    def print_stats(results, mode):
        ders = [r['der'] for r in results]
        detection_rates = [r['detection_rate'] for r in results]
        rtfs = [r['rtf'] for r in results]

        print(f"\n=== {mode.upper()} MODE SUMMARY ===")
        print(f"Average DER: {np.mean(ders):.3f} ± {np.std(ders):.3f}")
        print(f"Average Detection Rate: {np.mean(detection_rates):.3f} ± {np.std(detection_rates):.3f}")
        print(f"Average RTF: {np.mean(rtfs):.3f} ± {np.std(rtfs):.3f}")

        if mode == 'online':
            frame_times = [r['avg_frame_time'] for r in results]
            print(f"Average Frame Processing Time: {np.mean(frame_times):.5f}s ± {np.std(frame_times):.5f}s")


    print_stats(offline_results, 'offline')
    print_stats(online_results, 'online')

    # Plot results for first file
    print("\nPlotting results for first test file...")
    evaluator.plot_vad_comparison(test_files[0], online_results[0])
    return online_results, offline_results, test_files

if __name__ == "__main__":
    online_results, offline_results, test_files =  run_full_evaluation()