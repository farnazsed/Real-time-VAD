# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mBqIc7l6qeNFks7lymmGXbg-v0iyQOYQ
"""

!pip install pyannote.audio torchaudio matplotlib
!apt-get install libportaudio2

from huggingface_hub import notebook_login
notebook_login()

import torch
import numpy as np
from pyannote.audio import Pipeline
from google.colab import output
from IPython.display import display, Javascript, HTML, clear_output
import time
from ipywidgets import widgets
import queue
import threading
import matplotlib.pyplot as plt
from matplotlib.collections import PolyCollection

class ContinuousSpeechDetector:
    def __init__(self):
        # Audio configuration
        self.sample_rate = 16000
        self.chunk_size = 512  # 32ms chunks (processing granularity)
        self.vad_window = 2048  # 128ms window (VAD context)
        self.hangover_time = 0.2  # 200ms hangover
        self.last_speech_time = 0
        self.min_segment_length = 0.1  # Minimum speech segment duration

        # Initialize VAD pipeline
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/voice-activity-detection",
            use_auth_token=True
        ).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

        self.pipeline.instantiate({
            'onset': 0.7, 'offset': 0.5,
            'min_duration_on': self.min_segment_length,
            'min_duration_off': 0.2
        })

        # State variables
        self.is_speaking = False
        self.is_recording = False
        self.audio_buffer = np.zeros(self.vad_window, dtype=np.float32)
        self.full_audio = np.array([], dtype=np.float32)
        self.vad_results = []
        self.realtime_vad = []  # Stores chunk-level VAD decisions
        self.audio_queue = queue.Queue()
        self.buffer_position = 0

        # UI components
        self.status_display = widgets.Output()
        self.plot_display = widgets.Output()
        self.setup_ui()

        self.processing_thread = None

    def setup_ui(self):
        """Initialize user interface components"""
        self.start_button = widgets.Button(
            description="Start Detection",
            button_style='success',
            layout=widgets.Layout(width='150px')
        )
        self.stop_button = widgets.Button(
            description="Stop Detection",
            button_style='danger',
            layout=widgets.Layout(width='150px')
        )
        self.plot_button = widgets.Button(
            description="Show Results",
            button_style='info',
            layout=widgets.Layout(width='150px'),
            disabled=True
        )

        self.start_button.on_click(self.start_detection)
        self.stop_button.on_click(self.stop_detection)
        self.plot_button.on_click(self.show_results)

        display(widgets.HBox([self.start_button, self.stop_button, self.plot_button]))
        display(self.status_display)
        display(self.plot_display)

        with self.status_display:
            display(HTML("""
            <div style="font-size:24px; color:blue; text-align:center;
                        padding:20px; background-color:#e6f3ff; border-radius:10px;">
                Ready to start detection
            </div>
            """))

    def process_audio_chunks(self):
        """Process audio chunks with real-time VAD decisions"""
        while self.is_recording:
            try:
                audio_data = self.audio_queue.get(timeout=0.1)
                audio_array = np.array(audio_data, dtype=np.float32)

                # Store full recording
                self.full_audio = np.concatenate((self.full_audio, audio_array))

                # Update buffer (sliding window)
                self.audio_buffer[:-self.chunk_size] = self.audio_buffer[self.chunk_size:]
                self.audio_buffer[-self.chunk_size:] = audio_array

                # Make chunk-level VAD decision
                chunk_time = len(self.full_audio) / self.sample_rate
                is_speech = self._chunk_vad_decision()

                # Store decision with timestamp
                self.realtime_vad.append((chunk_time - (self.chunk_size/self.sample_rate),
                                        chunk_time,
                                        is_speech))

                # Update speech state with hangover
                if is_speech:
                    self.last_speech_time = chunk_time
                    if not self.is_speaking:
                        self.is_speaking = True
                        self._update_display()
                elif (chunk_time - self.last_speech_time) < self.hangover_time:
                    if not self.is_speaking:
                        self.is_speaking = True
                        self._update_display()
                else:
                    if self.is_speaking:
                        self.is_speaking = False
                        self._update_display()

            except queue.Empty:
                continue

    def _chunk_vad_decision(self):
        """Make VAD decision for current chunk using context window"""
        current_rms = np.sqrt(np.mean(self.audio_buffer**2))
        if current_rms < 0.01:  # Noise floor threshold
            return False

        try:
            with torch.no_grad():
                vad_out = self.pipeline({
                    "waveform": torch.from_numpy(self.audio_buffer).float().unsqueeze(0),
                    "sample_rate": self.sample_rate
                })

                # Consider it speech if any segment detected in window
                return any(True for _ in vad_out.get_timeline().support())

        except Exception as e:
            print(f"VAD error: {str(e)}")
            return False



    def _update_display(self):
        """Update the status display"""
        with self.status_display:
            clear_output(wait=True)
            if self.is_speaking:
                display(HTML("""
                <div style="font-size:48px; color:green; text-align:center;
                            padding:20px; background-color:#e6ffe6; border-radius:10px;">
                    SPEECH DETECTED
                </div>
                <div style="color:gray; text-align:center;">
                    Last update: {time}
                </div>
                """.format(time=time.strftime('%H:%M:%S'))))
            else:
                display(HTML("""
                <div style="font-size:48px; color:red; text-align:center;
                            padding:20px; background-color:#ffe6e6; border-radius:10px;">
                    SILENCE
                </div>
                <div style="color:gray; text-align:center;">
                    Last update: {time}
                </div>
                """.format(time=time.strftime('%H:%M:%S'))))

    def audio_callback(self, audio_data):
        """Callback for incoming audio data"""
        if self.is_recording:
            self.audio_queue.put(audio_data)

    def start_detection(self, button=None):
        """Start voice activity detection"""
        if not self.is_recording:
            self.is_recording = True
            self.is_speaking = False
            self.audio_buffer.fill(0)
            self.full_audio = np.array([], dtype=np.float32)
            self.vad_results = []
            self.plot_button.disabled = True

            self.processing_thread = threading.Thread(target=self.process_audio_chunks)
            self.processing_thread.start()

            self._update_display()
            self._start_microphone()

    def stop_detection(self, button=None):
        """Stop voice activity detection"""
        if self.is_recording:
            self.is_recording = False
            if self.processing_thread:
                self.processing_thread.join()

            with self.status_display:
                clear_output(wait=True)
                display(HTML("""
                <div style="font-size:24px; color:blue; text-align:center;
                            padding:20px; background-color:#e6f3ff; border-radius:10px;">
                    Detection Stopped
                </div>
                """))

            self.plot_button.disabled = False

    def show_results(self, button=None):
        """Display the waveform and VAD results with hangover processing"""
        if len(self.full_audio) == 0:
            with self.plot_display:
                clear_output(wait=True)
                print("No audio data to display")
            return

        with self.plot_display:
            clear_output(wait=True)

            duration = len(self.full_audio) / self.sample_rate
            time_axis = np.linspace(0, duration, len(self.full_audio))

            # 1. Merge overlapping/adjacent segments (including hangover periods)
            merged_segments = []
            for start, end in sorted(self.vad_results):
                if not merged_segments:
                    merged_segments.append([start, end])
                else:
                    last_start, last_end = merged_segments[-1]
                    if start <= (last_end + self.hangover_time):  # Apply hangover threshold
                        merged_segments[-1][1] = max(last_end, end)  # Extend segment
                    else:
                        merged_segments.append([start, end])

            # 2. Filter out very short segments
            min_segment_length = 0.1
            filtered_segments = [
                (start, end) for start, end in merged_segments
                if (end - start) >= min_segment_length
            ]

            # 3. Calculate speech statistics
            speech_duration = sum(end - start for start, end in filtered_segments)
            speech_percentage = (speech_duration / duration) * 100
            segment_count = len(filtered_segments)

            fig, (ax_wave, ax_stats) = plt.subplots(
                2, 1,
                figsize=(14, 8),
                gridspec_kw={'height_ratios': [3, 1]}
            )

            ax_wave.plot(time_axis, self.full_audio, 'b-', alpha=0.7, linewidth=0.5)

            if filtered_segments:
                verts = [
                    [(start, -1), (start, 1), (end, 1), (end, -1)]
                    for start, end in filtered_segments
                ]
                ax_wave.add_collection(
                    PolyCollection(verts, facecolors='green', alpha=0.3, edgecolors='none')
                )

            ax_wave.set_title(
                f"Waveform with VAD Results\n"
                f"Speech: {speech_percentage:.1f}% | Segments: {segment_count} | Hangover: {self.hangover_time*1000:.0f}ms"
            )
            ax_wave.set_ylabel("Amplitude")
            ax_wave.set_xlim(0, duration)
            ax_wave.grid(True, alpha=0.3)

            ax_stats.barh(
                ['Speech', 'Silence'],
                [speech_duration, duration - speech_duration],
                color=['green', 'red']
            )
            ax_stats.set_xlabel("Duration (seconds)")
            ax_stats.set_title("Speech/Silence Distribution")

            for i, (label, val) in enumerate(zip(
                ['Speech', 'Silence'],
                [speech_duration, duration - speech_duration]
            )):
                ax_stats.text(
                    val/2, i,
                    f"{val:.2f}s ({val/duration:.1%})",
                    ha='center', va='center',
                    color='white', weight='bold'
                )

            plt.tight_layout()
            plt.show()

            print("\n=== VAD PERFORMANCE ===")
            print(f"Total duration: {duration:.2f} seconds")
            print(f"Speech detected: {speech_duration:.2f}s ({speech_percentage:.1f}%)")
            print(f"Number of segments: {segment_count}")
            if segment_count > 0:
                avg_length = speech_duration / segment_count
                print(f"Average segment length: {avg_length:.2f}s")
            print(f"Hangover duration: {self.hangover_time:.3f}s")

    def show_results(self, button=None):
        """Display results with chunk-level and aggregated decisions"""
        if len(self.full_audio) == 0:
            with self.plot_display:
                clear_output(wait=True)
                print("No audio data to display")
            return

        with self.plot_display:
            clear_output(wait=True)

            duration = len(self.full_audio) / self.sample_rate
            time_axis = np.linspace(0, duration, len(self.full_audio))

            # Process real-time VAD decisions
            speech_segments = []
            current_segment = None

            for start, end, is_speech in self.realtime_vad:
                if is_speech:
                    if current_segment is None:
                        current_segment = [start, end]
                    else:
                        current_segment[1] = end
                else:
                    if current_segment is not None:
                        speech_segments.append(tuple(current_segment))
                        current_segment = None

            if current_segment is not None:
                speech_segments.append(tuple(current_segment))

            # Apply hangover and minimum duration
            filtered_segments = []
            for seg in speech_segments:
                start, end = seg
                # Extend segment with hangover
                end = min(end + self.hangover_time, duration)
                # Only keep if meets minimum duration
                if (end - start) >= self.min_segment_length:
                    filtered_segments.append((start, end))

            # Calculate statistics
            speech_duration = sum(end - start for start, end in filtered_segments)
            speech_percentage = (speech_duration / duration) * 100
            segment_count = len(filtered_segments)

            # Plotting
            fig, (ax_wave, ax_vad, ax_stats) = plt.subplots(
                3, 1,
                figsize=(14, 10),
                gridspec_kw={'height_ratios': [3, 1, 1]}
            )

            # 1. Waveform with speech regions
            ax_wave.plot(time_axis, self.full_audio, 'b-', alpha=0.7, linewidth=0.5)
            if filtered_segments:
                verts = [
                    [(start, -1), (start, 1), (end, 1), (end, -1)]
                    for start, end in filtered_segments
                ]
                ax_wave.add_collection(
                    PolyCollection(verts, facecolors='green', alpha=0.3, edgecolors='none')
                )
            ax_wave.set_title(f"Waveform with VAD Results (Aggregated)")
            ax_wave.set_ylabel("Amplitude")
            ax_wave.grid(True, alpha=0.3)

            # 2. Raw chunk-level decisions
            for start, end, is_speech in self.realtime_vad:
                color = 'green' if is_speech else 'red'
                ax_vad.barh('Chunk', end-start, left=start, color=color, alpha=0.5)
            ax_vad.set_title("Chunk-level VAD Decisions")
            ax_vad.set_xlim(0, duration)
            ax_vad.grid(True, alpha=0.3)

            # 3. Statistics
            ax_stats.barh(
                ['Speech', 'Silence'],
                [speech_duration, duration - speech_duration],
                color=['green', 'red']
            )
            ax_stats.set_xlabel("Duration (seconds)")
            for i, (label, val) in enumerate(zip(
                ['Speech', 'Silence'],
                [speech_duration, duration - speech_duration]
            )):
                ax_stats.text(
                    val/2, i,
                    f"{val:.2f}s ({val/duration:.1%})",
                    ha='center', va='center',
                    color='white', weight='bold'
                )

            plt.tight_layout()
            plt.show()

            print("\n=== VAD PERFORMANCE ===")
            print(f"Total duration: {duration:.2f} seconds")
            print(f"Speech detected: {speech_duration:.2f}s ({speech_percentage:.1f}%)")
            print(f"Number of segments: {segment_count}")
            if segment_count > 0:
                avg_length = speech_duration / segment_count
                print(f"Average segment length: {avg_length:.2f}s")
            print(f"Chunk size: {self.chunk_size/self.sample_rate*1000:.0f}ms")
            print(f"Hangover duration: {self.hangover_time:.3f}s")

    def _start_microphone(self):
        """Initialize microphone capture with JavaScript using chunk_size"""
        display(Javascript(f"""
        let audioContext, processor;

        async function startVAD() {{
            try {{
                const stream = await navigator.mediaDevices.getUserMedia({{
                    audio: {{
                        sampleRate: 16000,
                        noiseSuppression: true,
                        echoCancellation: true
                    }}
                }});

                audioContext = new AudioContext({{ sampleRate: 16000 }});
                const source = audioContext.createMediaStreamSource(stream);
                processor = audioContext.createScriptProcessor({self.chunk_size}, 1, 1);

                processor.onaudioprocess = (e) => {{
                    const data = e.inputBuffer.getChannelData(0);
                    google.colab.kernel.invokeFunction(
                        'process_audio',
                        [Array.from(data)],
                        {{}}
                    );
                }};

                source.connect(processor);
                processor.connect(audioContext.destination);

            }} catch (error) {{
                console.error("Microphone error:", error);
                alert("Microphone access denied. Please refresh and click Allow when prompted");
            }}
        }}

        window.keepAlive = setInterval(() => {{
            console.log("Keeping microphone alive");
        }}, 5000);

        startVAD();
        """))


def main():
    detector = ContinuousSpeechDetector()
    output.enable_custom_widget_manager()
    output.register_callback('process_audio', detector.audio_callback)

    display(HTML("""
    <div style="margin:20px 0; padding:10px; background:#f0f0f0; border-radius:5px;">
        <h3>Real-Time Chunk-Level VAD Processing</h3>
        <p>This version processes audio in {chunk_size}-sample chunks ({chunk_ms:.0f}ms) with:</p>
        <ol>
            <li>Instant chunk-level decisions</li>
            <li>{vad_window}-sample context window</li>
            <li>{hangover_time}s hangover period</li>
            <li>Real-time aggregation</li>
        </ol>
    </div>
    """.format(
        chunk_size=512,
        chunk_ms=512/16000*1000,
        vad_window=2048,
        hangover_time=0.2
    )))

if __name__ == "__main__":
    main()