# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mBqIc7l6qeNFks7lymmGXbg-v0iyQOYQ
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pyannote.audio torchaudio streamz matplotlib ipywidgets
!apt-get install libportaudio2

from huggingface_hub import notebook_login
notebook_login()

import torch
import numpy as np
from pyannote.audio import Pipeline, Model
from pyannote.core import Segment, SlidingWindow, SlidingWindowFeature as SWF
import torchaudio
from google.colab import files
import time
from IPython.display import Audio, display, clear_output
import matplotlib.pyplot as plt
from streamz import Stream
from typing import Tuple, List
from ipywidgets import widgets, Layout
import io

# %matplotlib inline
plt.rcParams["figure.figsize"] = (14, 6)
plt.rcParams["figure.facecolor"] = 'white'

class LiveVADVisualizer:
    def __init__(self, window_size=2.0, hop_size=0.2, latency=0.5, history_size=10):

        self.sample_rate = 16000
        self.window_size = window_size
        self.hop_size = hop_size
        self.latency = latency
        self.history_size = history_size


        self.pipeline = Pipeline.from_pretrained(
            "pyannote/voice-activity-detection",
            use_auth_token=True
        ).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

        self.pipeline.instantiate({
            'onset': 0.6,
            'offset': 0.4,
            'min_duration_on': 0.05,
            'min_duration_off': 0.05
        })

        self.fig, (self.ax_wave, self.ax_vad) = plt.subplots(2, 1, sharex=True)
        self.fig.tight_layout(pad=3.0)
        self.ax_wave.set_title("Live Audio Waveform")
        self.ax_vad.set_title("Voice Activity Detection")
        self.ax_vad.set_xlabel("Time (seconds)")
        self.ax_wave.set_ylabel("Amplitude")
        self.ax_vad.set_ylabel("Speech Probability")
        self.ax_vad.set_ylim(-0.1, 1.1)

        # Data buffers for real-time display
        self.waveform_buffer = np.array([])
        self.vad_buffer = np.array([])
        self.time_buffer = np.array([])
        self.speech_segments = []
        self.waveform = np.array([])

        self.full_waveform = np.array([])
        self.full_vad = np.array([])
        self.full_time = np.array([])

        self.status_text = widgets.HTML(
            value="<h3>Status: Waiting for audio...</h3>",
            layout=Layout(width='100%', height='60px')
        )
        display(self.status_text)

        # Initialize stream processing
        self.setup_stream_pipeline()

    def setup_stream_pipeline(self):
        """Set up the streaming processing pipeline"""
        self.source = Stream()

        (self.source
            .map(self.process_chunk)
            .accumulate(self.Aggregator(latency=self.latency),
                          returns_state=True, start=None)
            .sink(self.update_display)
        )

    def process_chunk(self, chunk: SWF) -> SWF:
        """Process an audio chunk through VAD"""
        waveform = torch.from_numpy(chunk.data.T).float()

        # Apply VAD
        with torch.no_grad():
            vad_out = self.pipeline({
                "waveform": waveform,
                "sample_rate": self.sample_rate
            })


        speech_prob = np.zeros((len(chunk), 1))
        for segment in vad_out.get_timeline().support():
            start_idx = int(segment.start * self.sample_rate)
            end_idx = int(segment.end * self.sample_rate)
            speech_prob[start_idx:end_idx] = 1.0

        return SWF(speech_prob, chunk.sliding_window)

    class Aggregator:

        def __init__(self, latency=0.5):
            self.latency = latency

        def __call__(self, internal_state, current_vad: SWF) -> Tuple[Tuple[float, List[SWF]], SWF]:

            if internal_state is None:
                internal_state = (0.0, [])

            # previous call led to the emission of aggregated scores up to time `delayed_time`
            # `past_buffers` is a rolling list of past buffers that we are going to aggregate
            delayed_time, past_buffers = internal_state

            # real time is the current end time of the audio buffer
            # (here, estimated from the end time of the VAD buffer)
            real_time = current_vad.extent.end

            # because we are only allowed `self.latency` seconds of latency, this call should
            # return aggregated scores for [delayed_time, real_time - latency] time range.
            required = Segment(delayed_time, real_time - self.latency)

            # to compute more robust scores, we will combine all buffers that have a non-empty
            # temporal intersection with required time range. we can get rid of the others as they
            # will no longer be needed as they are too far away in the past.
            past_buffers = [buf for buf in past_buffers if buf.extent.end > required.start] + [current_vad]


            # we aggregate all past buffers (but only on the 'required' region of interest)
            intersection = np.stack([buf.crop(required, fixed=required.duration) for buf in past_buffers])
            aggregation = np.mean(intersection, axis=0)

            # ... and wrap it into a self-contained SlidingWindowFeature (SWF) instance
            resolution = current_vad.sliding_window
            resolution = SlidingWindow(start=required.start,
                                      duration=resolution.duration,
                                      step=resolution.step)
            output = SWF(aggregation, resolution)

            # we update the internal state
            delayed_time = real_time - self.latency
            internal_state = (delayed_time, past_buffers)

            # ... and return the whole thing for next call to know where we are
            return internal_state, output

    def update_display(self, vad_result: SWF):

        current_time = vad_result.extent.end
        current_vad = vad_result.data.squeeze()

        # Update buffers for real-time display
        time_points = np.linspace(current_time - len(current_vad)/self.sample_rate,
                                current_time, len(current_vad))

        self.waveform_buffer = np.concatenate([self.waveform_buffer, vad_result.data.T.squeeze()])[-int(self.history_size*self.sample_rate):]
        self.vad_buffer = np.concatenate([self.vad_buffer, current_vad])[-int(self.history_size*self.sample_rate):]
        self.time_buffer = np.concatenate([self.time_buffer, time_points])[-int(self.history_size*self.sample_rate):]

        # Update full recording buffers
        self.full_waveform = np.concatenate([self.full_waveform, vad_result.data.T.squeeze()])
        self.full_vad = np.concatenate([self.full_vad, current_vad])
        self.full_time = np.concatenate([self.full_time, time_points])

        # Update speech segments
        self.update_speech_segments()

        clear_output(wait=True)
        display(self.status_text)

        self.ax_wave.clear()
        self.ax_wave.plot(self.time_buffer, self.waveform_buffer, 'b-', alpha=0.7, linewidth=0.5)
        self.ax_wave.set_title(f"Live Audio Waveform (showing last {self.history_size}s)")
        self.ax_wave.set_ylabel("Amplitude")

        # Highlight speech regions on waveform
        for seg in self.speech_segments:
            if seg[1] >= self.time_buffer[0] and seg[0] <= self.time_buffer[-1]:
                start = max(seg[0], self.time_buffer[0])
                end = min(seg[1], self.time_buffer[-1])
                self.ax_wave.axvspan(start, end, color='green', alpha=0.2)

        # Plot VAD results
        self.ax_vad.clear()
        self.ax_vad.plot(self.time_buffer, self.vad_buffer, 'r-', alpha=0.7, linewidth=1)
        self.ax_vad.fill_between(self.time_buffer, 0, self.vad_buffer, color='red', alpha=0.2)
        self.ax_vad.set_title("Voice Activity Detection")
        self.ax_vad.set_xlabel("Time (seconds)")
        self.ax_vad.set_ylabel("Speech Probability")
        self.ax_vad.set_ylim(-0.1, 1.1)

        # Draw threshold line
        self.ax_vad.axhline(0.5, color='gray', linestyle='--', alpha=0.5)

        plt.show()

        # Update status
        current_speech = np.mean(self.vad_buffer[-int(self.sample_rate*0.2):]) > 0.5  # 200ms lookback
        if current_speech:
            self.status_text.value = f"<h3 style='color:green;'>Status: SPEECH DETECTED ({time.strftime('%H:%M:%S')})</h3>"
        else:
            self.status_text.value = f"<h3 style='color:red;'>Status: silence ({time.strftime('%H:%M:%S')})</h3>"

    def update_speech_segments(self):
        """Update the list of continuous speech segments"""
        speech_mask = self.vad_buffer > 0.5
        changes = np.diff(speech_mask.astype(int))
        starts = np.where(changes == 1)[0]
        ends = np.where(changes == -1)[0]

        # Handle edge cases
        if speech_mask[0]:
            starts = np.insert(starts, 0, 0)
        if speech_mask[-1]:
            ends = np.append(ends, len(speech_mask)-1)

        new_segments = []
        for start, end in zip(starts, ends):
            seg_start = self.time_buffer[start]
            seg_end = self.time_buffer[end]
            new_segments.append((seg_start, seg_end))

        # Merge with existing segments if they overlap
        if not self.speech_segments:
            self.speech_segments = new_segments
        else:
            merged = []
            all_segments = sorted(self.speech_segments + new_segments)
            current_start, current_end = all_segments[0]

            for seg in all_segments[1:]:
                if seg[0] <= current_end:
                    current_end = max(current_end, seg[1])
                else:
                    merged.append((current_start, current_end))
                    current_start, current_end = seg
            merged.append((current_start, current_end))

            self.speech_segments = merged[-10:]  # Keep last 10 segments for display

    def show_final_plot(self):
        """Display the final complete plot with waveform and VAD results"""
        from matplotlib.collections import PolyCollection
        clear_output(wait=True)

        fig, (ax_wave, ax_vad) = plt.subplots(2, 1, sharex=True, figsize=(14, 8))
        fig.tight_layout(pad=3.0)

        # Calculate speech segments
        speech_mask = self.full_vad > 0.5
        changes = np.diff(speech_mask.astype(int))
        starts = np.where(changes == 1)[0]
        ends = np.where(changes == -1)[0]

        if speech_mask[0]:
            starts = np.insert(starts, 0, 0)
        if speech_mask[-1]:
            ends = np.append(ends, len(speech_mask)-1)

        # Convert to time values
        speech_segments = []
        for start, end in zip(starts, ends):
            seg_start = self.full_time[start]
            seg_end = self.full_time[end]
            speech_segments.append((seg_start, seg_end))

        # Calculate statistics
        total_duration = len(self.full_waveform) / self.sample_rate
        speech_duration = np.sum(self.full_vad > 0.5) / self.sample_rate
        speech_percentage = (speech_duration / total_duration) * 100

        ax_wave.plot(self.full_time, self.full_waveform, 'b-', alpha=0.7, linewidth=0.5)

        verts = [
            [(start, -1), (start, 1), (end, 1), (end, -1)]
            for start, end in speech_segments
        ]
        if verts:
            ax_wave.add_collection(
                PolyCollection(verts, facecolors='green', alpha=0.3, edgecolors='none')
            )

        ax_wave.set_title(
            f"Audio Waveform with Speech Regions\nSpeech: {speech_percentage:.1f}% | Segments: {len(speech_segments)}"
        )
        ax_wave.set_ylabel("Amplitude")
        ax_wave.grid(True, alpha=0.3)
        ax_wave.set_ylim(-1.1, 1.1)

        ax_vad.plot(self.full_time, self.full_vad, 'r-', alpha=0.7, linewidth=1)
        ax_vad.fill_between(self.full_time, 0, self.full_vad, color='red', alpha=0.2)
        ax_vad.set_title("Voice Activity Detection Probability")
        ax_vad.set_xlabel("Time (seconds)")
        ax_vad.set_ylabel("Probability")
        ax_vad.set_ylim(-0.1, 1.1)
        ax_vad.axhline(0.5, color='gray', linestyle='--', alpha=0.5)
        ax_vad.grid(True, alpha=0.3)


        plt.show()

        print("\n=== FINAL RESULTS ===")
        print(f"Total audio duration: {total_duration:.2f}s")
        print(f"Speech duration: {speech_duration:.2f}s ({speech_percentage:.1f}%)")
        print(f"Number of speech segments: {len(speech_segments)}")
        if len(speech_segments) > 0:
            avg_length = np.mean([end-start for start,end in speech_segments])
            print(f"Average speech segment length: {avg_length:.2f}s")
        else:
            print("No speech segments detected")

    def plot_initial_waveform(self, waveform, sample_rate):
        """Plot the original audio waveform before any processing"""
        time_axis = np.arange(len(waveform)) / sample_rate
        self.waveform = waveform


        plt.figure(figsize=(14, 4))
        plt.plot(time_axis, waveform, 'b-', alpha=0.7, linewidth=0.5)
        plt.title("Original Audio Waveform")
        plt.xlabel("Time (seconds)")
        plt.ylabel("Amplitude")
        plt.grid(True, alpha=0.3)
        plt.show()


    def process_audio_file(self, file_path):
        """Process an audio file in a streaming fashion"""
        waveform, sample_rate = torchaudio.load(file_path)

        # Convert to mono and resample if needed
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        if sample_rate != self.sample_rate:
            resampler = torchaudio.transforms.Resample(
                orig_freq=sample_rate,
                new_freq=self.sample_rate
            )
            waveform = resampler(waveform)



        # Process in chunks
        chunk_size = int(self.window_size * self.sample_rate)
        hop_size = int(self.hop_size * self.sample_rate)
        total_samples = waveform.shape[1]

        self.status_text.value = "<h3 style='color:blue;'>Status: Processing audio...</h3>"

        for start in range(0, total_samples - chunk_size + 1, hop_size):
            end = start + chunk_size
            chunk = waveform[:, start:end]

            # Create SlidingWindowFeature for the chunk
            time_resolution = SlidingWindow(
                start=start / self.sample_rate,
                duration=1./self.sample_rate,
                step=1./self.sample_rate
            )
            swf_chunk = SWF(chunk.numpy().T, time_resolution)

            # Feed to stream
            self.source.emit(swf_chunk)
            time.sleep(self.hop_size)

        self.status_text.value = "<h3 style='color:blue;'>Status: Processing complete!</h3>"

        self.show_final_plot()
        self.plot_initial_waveform(waveform.squeeze().numpy(), self.sample_rate)


if __name__ == "__main__":
    print("Please upload an audio file:")
    uploaded = files.upload()
    audio_file = next(iter(uploaded.keys()))
    vad_visualizer = LiveVADVisualizer(window_size=2.0, hop_size=0.2, latency=0.5, history_size=10)
    vad_visualizer.process_audio_file(audio_file)