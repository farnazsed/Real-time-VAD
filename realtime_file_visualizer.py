# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mBqIc7l6qeNFks7lymmGXbg-v0iyQOYQ
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install pyannote.audio torchaudio streamz matplotlib ipywidgets
!apt-get install libportaudio2

from huggingface_hub import notebook_login
notebook_login()

import torch
import numpy as np
from pyannote.audio import Pipeline, Model
from pyannote.core import Segment, SlidingWindow, SlidingWindowFeature as SWF
import torchaudio
from google.colab import files
import time
from IPython.display import Audio, display, clear_output
import matplotlib.pyplot as plt
from streamz import Stream
from typing import Tuple, List
from ipywidgets import widgets, Layout
import io
from matplotlib.collections import PolyCollection

# Set up plotting
%matplotlib inline
plt.rcParams["figure.figsize"] = (14, 8)
plt.rcParams["figure.facecolor"] = 'white'

class LiveVADVisualizer:
    def __init__(self, window_size=0.5, hop_size=0.05, latency=0.2, history_size=5):
        self.sample_rate = 16000
        self.window_size = window_size
        self.hop_size = hop_size
        self.latency = latency
        self.history_size = history_size

        # Initialize data buffers
        self.waveform_buffer = np.array([])
        self.vad_buffer = np.array([])
        self.time_buffer = np.array([])
        self.speech_segments = []

        # For storing complete recording
        self.full_waveform = np.array([])
        self.full_vad = np.array([])
        self.full_time = np.array([])

        # Original audio track
        self.original_waveform = None
        self.original_sample_rate = None
        self.original_time = None

        # Initialize VAD pipeline
        self.pipeline = Pipeline.from_pretrained(
            "pyannote/voice-activity-detection",
            use_auth_token=True
        ).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

        self.pipeline.instantiate({
            'onset': 0.5,  # More sensitive threshold
            'offset': 0.3,
            'min_duration_on': 0.1,
            'min_duration_off': 0.1
        })

        # Status widget
        self.status_text = widgets.HTML(
            value="<h3>Status: Waiting for audio...</h3>",
            layout=Layout(width='100%', height='60px')
        )
        display(self.status_text)

        # Initialize stream processing
        self.setup_stream_pipeline()

    def setup_stream_pipeline(self):
        """Set up the streaming processing pipeline"""
        self.source = Stream()

        (self.source
            .map(self.process_chunk)
            .accumulate(self.Aggregator(latency=self.latency),
                      returns_state=True, start=None)
            .sink(self.update_display)
        )

    def process_chunk(self, chunk: SWF) -> SWF:
        """Process an audio chunk through VAD"""
        waveform = torch.from_numpy(chunk.data.T).float()

        # Apply VAD
        with torch.no_grad():
            vad_out = self.pipeline({
                "waveform": waveform,
                "sample_rate": self.sample_rate
            })

        speech_prob = np.zeros((len(chunk), 1))
        for segment in vad_out.get_timeline().support():
            start_idx = int(segment.start * self.sample_rate)
            end_idx = int(segment.end * self.sample_rate)
            speech_prob[start_idx:end_idx] = 1.0

        return SWF(speech_prob, chunk.sliding_window)

    class Aggregator:
        def __init__(self, latency=0.5):
            self.latency = latency

        def __call__(self, internal_state, current_vad: SWF) -> Tuple[Tuple[float, List[SWF]], SWF]:
            if internal_state is None:
                internal_state = (0.0, [])

            delayed_time, past_buffers = internal_state
            real_time = current_vad.extent.end
            required = Segment(delayed_time, real_time - self.latency)

            past_buffers = [buf for buf in past_buffers if buf.extent.end > required.start] + [current_vad]

            intersection = np.stack([buf.crop(required, fixed=required.duration) for buf in past_buffers])
            aggregation = np.mean(intersection, axis=0)

            resolution = current_vad.sliding_window
            resolution = SlidingWindow(start=required.start,
                                    duration=resolution.duration,
                                    step=resolution.step)
            output = SWF(aggregation, resolution)

            delayed_time = real_time - self.latency
            internal_state = (delayed_time, past_buffers)

            return internal_state, output

    def update_display(self, vad_result: SWF):
        current_time = vad_result.extent.end
        current_vad = vad_result.data.squeeze()

        # Update buffers
        time_points = np.linspace(current_time - len(current_vad)/self.sample_rate,
                                current_time, len(current_vad))

        self.waveform_buffer = np.concatenate([self.waveform_buffer, vad_result.data.T.squeeze()])[-int(self.history_size*self.sample_rate):]
        self.vad_buffer = np.concatenate([self.vad_buffer, current_vad])[-int(self.history_size*self.sample_rate):]
        self.time_buffer = np.concatenate([self.time_buffer, time_points])[-int(self.history_size*self.sample_rate):]

        # Update full recording buffers
        self.full_waveform = np.concatenate([self.full_waveform, vad_result.data.T.squeeze()])
        self.full_vad = np.concatenate([self.full_vad, current_vad])
        self.full_time = np.concatenate([self.full_time, time_points])

        # Update speech segments
        self.update_speech_segments()

        # Clear previous output
        clear_output(wait=True)

        # Create new figure for each update (Colab workaround)
        fig, (ax_original, ax_wave, ax_vad) = plt.subplots(3, 1, figsize=(14, 10))

        # 1. Original waveform plot
        if self.original_waveform is not None:
            ax_original.plot(self.original_time, self.original_waveform, 'b-', alpha=0.5, linewidth=0.5)
            for seg_start, seg_end in self.speech_segments:
                ax_original.axvspan(seg_start, seg_end, color='green', alpha=0.3)
            ax_original.axvline(current_time, color='red', linestyle='--', alpha=0.7)
            ax_original.set_title(f"Original Audio (Time: {current_time:.2f}s)")
            ax_original.set_ylabel("Amplitude")
            ax_original.grid(True, alpha=0.2)

        # 2. Live waveform plot
        ax_wave.plot(self.time_buffer, self.waveform_buffer, 'b-', alpha=0.7, linewidth=0.5)
        for seg in self.speech_segments:
            if seg[1] >= self.time_buffer[0] and seg[0] <= self.time_buffer[-1]:
                ax_wave.axvspan(max(seg[0], self.time_buffer[0]),
                              min(seg[1], self.time_buffer[-1]),
                              color='green', alpha=0.2)
        ax_wave.set_title(f"Live Audio (Last {self.history_size}s)")
        ax_wave.set_ylabel("Amplitude")
        ax_wave.grid(True, alpha=0.2)

        # 3. VAD plot
        ax_vad.plot(self.time_buffer, self.vad_buffer, 'r-', alpha=0.7, linewidth=1)
        ax_vad.fill_between(self.time_buffer, 0, self.vad_buffer,
                         where=(self.vad_buffer > 0.5),
                         color='red', alpha=0.2)
        ax_vad.set_title("Voice Activity Detection")
        ax_vad.set_xlabel("Time (seconds)")
        ax_vad.set_ylabel("Probability")
        ax_vad.set_ylim(-0.1, 1.1)
        ax_vad.axhline(0.5, color='gray', linestyle='--', alpha=0.5)
        ax_vad.grid(True, alpha=0.2)

        # Update status
        current_speech = np.mean(self.vad_buffer[-int(self.sample_rate*0.2):]) > 0.5
        status_color = 'green' if current_speech else 'red'
        status_text = "SPEECH DETECTED" if current_speech else "silence"
        self.status_text.value = f"<h3 style='color:{status_color};'>Status: {status_text} ({time.strftime('%H:%M:%S')})</h3>"

        # Display everything
        display(fig)
        display(self.status_text)
        plt.close(fig)  # Prevent figure duplication

    def update_speech_segments(self):
        """Update the list of continuous speech segments"""
        speech_mask = self.vad_buffer > 0.5
        changes = np.diff(speech_mask.astype(int))
        starts = np.where(changes == 1)[0]
        ends = np.where(changes == -1)[0]

        if speech_mask[0]:
            starts = np.insert(starts, 0, 0)
        if speech_mask[-1]:
            ends = np.append(ends, len(speech_mask)-1)

        new_segments = []
        for start, end in zip(starts, ends):
            seg_start = self.time_buffer[start]
            seg_end = self.time_buffer[end]
            new_segments.append((seg_start, seg_end))

        if not self.speech_segments:
            self.speech_segments = new_segments
        else:
            merged = []
            all_segments = sorted(self.speech_segments + new_segments)
            current_start, current_end = all_segments[0]

            for seg in all_segments[1:]:
                if seg[0] <= current_end:
                    current_end = max(current_end, seg[1])
                else:
                    merged.append((current_start, current_end))
                    current_start, current_end = seg
            merged.append((current_start, current_end))

            self.speech_segments = merged[-10:]

    def show_final_plot(self):
        """Display final results"""
        fig, (ax_orig, ax_vad) = plt.subplots(2, 1, figsize=(14, 8))

        # 1. Original waveform with speech regions
        ax_orig.plot(self.original_time, self.original_waveform, 'b-', alpha=0.7, linewidth=0.7)
        for seg_start, seg_end in self.speech_segments:
            ax_orig.axvspan(seg_start, seg_end, color='green', alpha=0.3)
        ax_orig.set_title("Original Audio with Detected Speech Regions")
        ax_orig.set_ylabel("Amplitude")
        ax_orig.grid(True, alpha=0.2)

        # 2. VAD results
        ax_vad.plot(self.full_time, self.full_vad, 'r-', alpha=0.7, linewidth=1)
        ax_vad.fill_between(self.full_time, 0, self.full_vad,
                          where=(self.full_vad > 0.5),
                          color='red', alpha=0.2)
        ax_vad.set_title("Voice Activity Detection Probability")
        ax_vad.set_xlabel("Time (seconds)")
        ax_vad.set_ylabel("Probability")
        ax_vad.set_ylim(-0.1, 1.1)
        ax_vad.axhline(0.5, color='gray', linestyle='--', alpha=0.5)
        ax_vad.grid(True, alpha=0.2)

        # Add statistics
        total_duration = len(self.original_waveform) / self.original_sample_rate
        speech_duration = sum(end-start for start,end in self.speech_segments)
        speech_percentage = (speech_duration / total_duration) * 100
        fig.suptitle(f"Total: {total_duration:.2f}s | Speech: {speech_duration:.2f}s ({speech_percentage:.1f}%) | Segments: {len(self.speech_segments)}",
                    y=1.02)

        plt.tight_layout()
        plt.show()

    def process_audio_file(self, file_path):
        """Process an audio file with real-time visualization"""
        waveform, sample_rate = torchaudio.load(file_path)

        # Store original waveform
        self.original_waveform = waveform.squeeze().numpy()
        self.original_sample_rate = sample_rate
        self.original_time = np.arange(len(self.original_waveform)) / sample_rate

        # Convert to mono and resample if needed
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        if sample_rate != self.sample_rate:
            resampler = torchaudio.transforms.Resample(
                orig_freq=sample_rate,
                new_freq=self.sample_rate
            )
            waveform = resampler(waveform)

        # Process in chunks
        chunk_size = int(self.window_size * self.sample_rate)
        hop_size = int(self.hop_size * self.sample_rate)
        total_samples = waveform.shape[1]

        self.status_text.value = "<h3 style='color:blue;'>Status: Processing audio...</h3>"

        for start in range(0, total_samples - chunk_size + 1, hop_size):
            end = start + chunk_size
            chunk = waveform[:, start:end]

            # Create SlidingWindowFeature for the chunk
            time_resolution = SlidingWindow(
                start=start / self.sample_rate,
                duration=1./self.sample_rate,
                step=1./self.sample_rate
            )
            swf_chunk = SWF(chunk.numpy().T, time_resolution)

            # Feed to stream
            self.source.emit(swf_chunk)

            # Add small delay to control update rate
            time.sleep(self.hop_size * 0.9)

        self.status_text.value = "<h3 style='color:blue;'>Status: Processing complete!</h3>"
        self.show_final_plot()

# Main execution
if __name__ == "__main__":
    print("Please upload an audio file:")
    uploaded = files.upload()
    audio_file = next(iter(uploaded.keys()))
    vad_visualizer = LiveVADVisualizer(window_size=0.5, hop_size=0.05)
    vad_visualizer.process_audio_file(audio_file)
